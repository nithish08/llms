{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d1edda",
   "metadata": {},
   "source": [
    "**Table of Contents**  \n",
    "\n",
    "- [Imports](#$imports)    \n",
    "- [Load Data](#load-data)    \n",
    "- [BiGram Model to Generate Text](#bigram-model)    \n",
    "- [Mathematical Trick in Self-Attention](#math-trick-self-attention)    \n",
    "- [Self-Attention Ad-Hoc Implementation](#self-attention-adhoc)    \n",
    "- [Scaling Weights for Unit Variance](#scaling-weights-unit-variance)    \n",
    "- [Bigram Model with Single Self-Attention Head](#bigram-single-head)    \n",
    "- [Bigram Model with Multiple Self-Attention Heads](#bigram-multi-head)    \n",
    "- [Bigram Model with Multiple Self-Attention Blocks](#bigram-multi-block)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"imports\"></a> Imports  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # we use PyTorch: https://pytorch.org\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"load-data\"></a> Load Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd51a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n",
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# read it in to inspect it\n",
    "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [\n",
    "    stoi[c] for c in s\n",
    "]  # encoder: take a string, output a list of integers\n",
    "decode = lambda l: \"\".join(\n",
    "    [itos[i] for i in l]\n",
    ")  # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))\n",
    "\n",
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(\n",
    "    data[:1000]\n",
    ")  # the 1000 characters we looked at earier will to the GPT look like this\n",
    "\n",
    "\n",
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "block_size = 8\n",
    "train_data[: block_size + 1]\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "block_size = 8\n",
    "train_data[: block_size + 1]\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1 : block_size + 1]\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4  # how many independent sequences will we process in parallel?\n",
    "block_size = 8  # what is the maximum context length for predictions?\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"----\")\n",
    "\n",
    "for b in range(batch_size):  # batch dimension\n",
    "    for t in range(block_size):  # time dimension\n",
    "        context = xb[b, : t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"bigram-model\"></a> BiGram Model to Generate Text  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0878e97-be96-4061-b31c-b4234fef55e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGram(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BiGram, self).__init__()\n",
    "        self.emb_layer = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # x - > (B,T)\n",
    "        logits = self.emb_layer(x)  # B, T, C\n",
    "        B, T, C = logits.shape\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx B, T\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            # take the last one\n",
    "            logits = logits[:, -1, :]  # B,C\n",
    "            # -1 here in the time direction means that only\n",
    "            # previous word is considered.\n",
    "            probs = F.softmax(logits, -1)\n",
    "            new_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, new_idx], axis=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d37b6e-162f-4a71-8766-64af51a9d1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65]) tensor(5.0364, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "lfJeukRuaRJKXAYtXzfJ:HEPiu--sDioi;ILCo3pHNTmDwJsfheKRxZCFs\n",
      "lZJ XQc?:s:HEzEnXalEPklcPU cL'DpdLCafBheH\n"
     ]
    }
   ],
   "source": [
    "bigram = BiGram()\n",
    "logits, loss = bigram(xb, yb)\n",
    "\n",
    "print(logits.shape, loss)  # check logits shape and loss\n",
    "print(\n",
    "    decode(\n",
    "        bigram.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706439b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nithish/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/nithish/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.545020580291748\n",
      "\n",
      "tcQDHEXsLEocK;tgQ;y?q Ew$D-MkpJuerOn$vSmct'mOdwdw-Thoek$AqS:UCRQPauiu XTus!kRuBHeA-'uOm'N\n",
      "tp'SrP&!NgRjK3m'mn: mOYxHci&'Wdgersq-E-:Zty3bBMf,w:V.m$osh$kot:LTziacyd\n",
      "RuibLE?KXyMq$k\n",
      "LLKD-qWy&h.Su'opx'NxVTyFCdNhpZL-x:ZLaT ym&o&IYBfzydo?MQgFZiNAUg;QA.XajBE$LwU!nBf;kGMcZPw fBjop.EVLcBzg;cPf;.qqAEmy,r \n",
      "sZhV-suIRugljoS.xvPXjGtfH\n",
      "w:otygZQNWLCoiopLwkZEhOVe:ci,NQJ.F\n",
      "kZcQa?qIJKBYgvhw HEzyE-JWtIJOJKEnL z,-sfINT VI'rKlXhvsgwXjz-J:FT'3pYxxmjgVyNU.O?eSfb.jg!!dlrENRFKzjtzakbezn$PgGUKXr!WwkZz?TgSbBhwGW,iximRENU Bf$\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(bigram.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(100):  # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    # evaluate the loss\n",
    "    logits, loss = bigram(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "\n",
    "print(\n",
    "    decode(\n",
    "        bigram.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"math-trick-self-attention\"></a> Mathematical Trick in Self-Attention  \n",
    "\n",
    "using lower triangular matrix for weighted aggregated.\n",
    "lower triangular matrix is used for concept of not seeing future token when predicting a toke at time step \"t\"\n",
    "weights are learned to give different weights for different tokens that appear till the time t-1 when predicting for \"t\"\n",
    "\n",
    "3 methods for doing causal multiplication are demonstrated below\n",
    "1. straightfoward matrix mul\n",
    "2. lower triangular matrix\n",
    "3. softmax + triangular matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c0aef5-a8fe-47e3-9260-a5f00a8d7801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# method-1 straightforward matric multiplication\n",
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b=\")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c=\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9badb59-35ad-4eb3-aa7e-9a85141f663c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method-2: lower triangular matrix\n",
    "\n",
    "# consider the following toy example:\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa4565-7b9c-4340-acfb-5796f13bdff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, : t + 1]  # (t,C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c189f-e520-4f03-85ae-80f5a37be1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the method of weighted aggregation\n",
    "# B,T,C finally we want to get B,T,C with weighted average\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1285ec55-ec27-4329-8117-88f885e2d43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e99101b1-4a1b-4b02-a73b-3a0b2fe6c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow2 = wei @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee756d9c-8742-4e1f-baf5-df22d17c2d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f998c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method-3\n",
    "# Using the softmax\n",
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"self-attention-adhoc\"></a> Self-Attention Ad-Hoc Implementation  \n",
    "\n",
    "\n",
    "Self attention\n",
    "for each tokens there is\n",
    "1. Query\n",
    "2. Key\n",
    "3. Value\n",
    "\n",
    "Query and key interact with each other and finally value is a representaion of token added at the end.\n",
    "When query and key match their activation is high.\n",
    "\n",
    "In higher level\n",
    "query - what I want\n",
    "key - what I have\n",
    "value - a represenation of token with small dense layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ffd1f-81dd-4fce-a87a-5552d029779e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# version 4 : implement self attention\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32  # batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)  # B, T, head_size\n",
    "q = query(x)  # B, T, head_size\n",
    "v = value(x)  # B, T, head_size\n",
    "\n",
    "\n",
    "wei = q @ k.transpose(-2, -1)  # B, T, T\n",
    "# each time step interaction with other time step\n",
    "\n",
    "trill = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(trill == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# this makes sure the causal connection in wei\n",
    "\n",
    "\n",
    "# finally take the causal wei and matmul with v\n",
    "output = wei @ v\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55aceab",
   "metadata": {},
   "source": [
    "\n",
    "Notes:\n",
    "\n",
    "1. Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "1. There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "1. Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "1. In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "1. \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "1. \"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"scaling-weights-unit-variance\"></a> Scaling Weights for Unit Variance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ea50a-ccf8-44e6-8315-2ac7db8e164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without scaling\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b38a96-144f-47f2-90fb-3a94f966e0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0449), tensor(1.0700), tensor(17.4690))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var(), q.var(), wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb457f6-8a8a-4d2f-b346-888d733af65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with scaled variance\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247d2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9006), tensor(1.0037), tensor(0.9957))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var(), q.var(), wei.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"bigram-single-head\"></a> Bigram Model with Single Self-Attention Head  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46b057-5fa0-46b9-b3ef-a5539e767fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, input_size, head_size):\n",
    "        super(Head, self).__init__()\n",
    "        self.key = nn.Linear(input_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(input_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(input_size, head_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # B, T, head_size\n",
    "        q = self.query(x)  # B, T, head_size\n",
    "        wei = q @ k.transpose(-2, -1)  # B, T, T\n",
    "        trill = torch.tril(torch.ones(T, T))\n",
    "        wei = wei.masked_fill(trill == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)  # B, T, T\n",
    "\n",
    "        v = self.value(x)  # B, T, head_size\n",
    "        output = wei @ v  # B, T, head_size\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a30d6f-f62e-4758-a481-9b7793eccf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGramWithSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BiGramWithSelfAttention, self).__init__()\n",
    "        emb_size = 16\n",
    "        self.token_embed_table = nn.Embedding(vocab_size, emb_size)\n",
    "        self.position_embed_table = nn.Embedding(block_size, emb_size)\n",
    "        self.sa_head = Head(emb_size, emb_size)\n",
    "        self.lm_head = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # x - > (B,T)\n",
    "        B, T = x.shape\n",
    "        token_embeddings = self.token_embed_table(x)  # B, T, emb_size\n",
    "        positional_emebddings = self.position_embed_table(\n",
    "            torch.arange(T)\n",
    "        )  # B,T, emb_size\n",
    "        logits = (\n",
    "            token_embeddings + positional_emebddings\n",
    "        )  # B,T, emb_size #positional expanded and added\n",
    "        logits = self.sa_head(logits)\n",
    "        logits = self.lm_head(logits)  # B, T, vocab_size\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(B * T, -1)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx B, T\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # take the last one\n",
    "            logits = logits[:, -1, :]  # B,C\n",
    "            probs = F.softmax(logits, -1)\n",
    "            new_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, new_idx], axis=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e3309063-d3a6-41de-9007-3ed3d66d4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BiGramWithSelfAttention()\n",
    "logits, loss = m(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c9c92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4724979400634766\n",
      "\n",
      "INERTOLUF:\n",
      "D cot\n",
      "Heth hens ithit thyennod qut hat.\n",
      "\n",
      "lfmilsd co I ugo hres wy he utr nse an adt mothico athalnson pall yoven ICELOLU:\n",
      "G ce th!\n",
      "Wavilleire sir\n",
      "Bor ko lerd Whers movee.\n",
      "\n",
      "ciulth in:\n",
      "A thouingin mper he yillatw sseef tes Pak com, yod?\n",
      ":\n",
      "MIml\n",
      "Whe.\n",
      "MI:\n",
      "Sy me\n",
      "Cofto T'se yy m'ghatthe himowour mis I soat om, lans,\n",
      "I o fee cor, bud\n",
      "Yed!\n",
      "C st p fes kas ro Ror sablomee vesyullir tretopoteut then ct ronlt hem.\n",
      "\n",
      "Y hokr bee jeecowis igple spo;\n",
      "Liconen hekeand.\n",
      "\n",
      "O:\n",
      "MENNus'th thth per orowouns, I \n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(5000):  # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "print(\n",
    "    decode(\n",
    "        m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"bigram-multi-head\"></a> Bigram Model with Multiple Self-Attention Heads  \n",
    "\n",
    "\n",
    "- multiple heads of self attention running in parallel\n",
    "- All of them can be run parallely.\n",
    "- concat all outputs in channel dimension finally.\n",
    "- Communication channel is self attention, each communicate channel can learn something different\n",
    "- This is something like grouped convolution. (This is multi head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7dd21-f0a0-4e4b-8d41-83db573ade74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSelfAttentionHeads(nn.Module):\n",
    "    def __init__(self, input_size, head_size, num_heads):\n",
    "        super(MultiSelfAttentionHeads, self).__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Head(input_size, head_size // num_heads) for i in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        outputs = [head(x) for head in self.heads]\n",
    "        outputs = torch.concat(outputs, dim=-1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c353aa-2bd5-4bc0-a6c6-cd93a5934606",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGramWithMultiHeadSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BiGramWithMultiHeadSelfAttention, self).__init__()\n",
    "        emb_size = 16\n",
    "        self.token_embed_table = nn.Embedding(vocab_size, emb_size)\n",
    "        self.position_embed_table = nn.Embedding(block_size, emb_size)\n",
    "        self.sa_head = MultiSelfAttentionHeads(emb_size, emb_size, num_heads=4)\n",
    "        self.lm_head = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # x - > (B,T)\n",
    "        B, T = x.shape\n",
    "        token_embeddings = self.token_embed_table(x)  # B, T, emb_size\n",
    "        positional_emebddings = self.position_embed_table(torch.arange(T))  # B,T\n",
    "        logits = (\n",
    "            token_embeddings + positional_emebddings\n",
    "        )  # B,T, emb_size #positional expanded and added\n",
    "        logits = self.sa_head(logits)\n",
    "        logits = self.lm_head(logits)  # B, T, vocab_size\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(B * T, -1)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx B, T\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # take the last one\n",
    "            logits = logits[:, -1, :]  # B,C\n",
    "            probs = F.softmax(logits, -1)\n",
    "            new_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, new_idx], axis=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44503e8d-6299-42d0-b2ab-50d38ecf2164",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BiGramWithMultiHeadSelfAttention()\n",
    "logits, loss = m(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb492f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.363037109375\n",
      "\n",
      "HI HAMICAMofur beol lamasiche weat\n",
      "B:\n",
      "Ye haist hesthord my madulve in laqyomin ceerour:\n",
      "San;\n",
      "D of holthe. nuse\n",
      "Cemad-nve ipsttesth sud I dy shell-Fbt?\n",
      "\n",
      "I,\n",
      "LDUCDod AONESOLEFIEDFO:\n",
      "Te\n",
      "CyA IDut Poto che,\n",
      "Why leilgdecoos macourenas, of thelelie I my I sreg menl ost bamyne withent moold to oond sotr,\n",
      "Wo towigs\n",
      "\n",
      "Mulit.\n",
      "R'-zan thabeow por puid the.\n",
      "oo: I he pefce ang Xan of gow, pulere, the your sowes, tim Yon the nohe wirse for so it, bou his sorgeand noolt, whee as wir woos;\n",
      "Ang.\n",
      "\n",
      "TAng, I theto avip \n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(5000):  # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "print(\n",
    "    decode(\n",
    "        m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"bigram-multi-block\"></a> Bigram Model with Multiple Self-Attention Blocks  \n",
    "\n",
    "Block in this case are MultiHeadAttention + feed-feedforward networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895c904-9ef1-459c-ba1b-0c3fda570d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, input_size, head_size, num_heads):\n",
    "        super(Block, self).__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Head(input_size, head_size // num_heads) for i in range(num_heads)]\n",
    "        )\n",
    "        self.ffn = nn.Linear(head_size, head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        outputs = [head(x) for head in self.heads]\n",
    "        outputs = torch.concat(outputs, dim=-1)\n",
    "        outputs = self.ffn(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf252a61-c67a-43c8-8fce-9bf3ba071359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGramWithMultiHeadSelfAttentionSingleBlock(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BiGramWithMultiHeadSelfAttentionSingleBlock, self).__init__()\n",
    "        emb_size = 16\n",
    "        self.token_embed_table = nn.Embedding(vocab_size, emb_size)\n",
    "        self.position_embed_table = nn.Embedding(block_size, emb_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(emb_size, emb_size, num_heads=4),\n",
    "            Block(emb_size, emb_size, num_heads=4),\n",
    "            Block(emb_size, emb_size, num_heads=4),\n",
    "            Block(emb_size, emb_size, num_heads=4),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # x - > (B,T)\n",
    "        B, T = x.shape\n",
    "        token_embeddings = self.token_embed_table(x)  # B, T, emb_size\n",
    "        positional_emebddings = self.position_embed_table(torch.arange(T))  # B,T\n",
    "        logits = (\n",
    "            token_embeddings + positional_emebddings\n",
    "        )  # B,T, emb_size #positional expanded and added\n",
    "        logits = self.blocks(logits)\n",
    "        logits = self.lm_head(logits)  # B, T, vocab_size\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(B * T, -1)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        B, T = idx.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # take the last one\n",
    "            logits = logits[:, -1, :]  # B,C\n",
    "            probs = F.softmax(logits, -1)\n",
    "            new_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, new_idx], axis=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13720d-387f-43eb-ac89-d2a5c548051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.624037981033325\n",
      "\n",
      "TPyi:\n",
      "QFannh gins kraur av yanr sren fans ocer linn\n",
      "We tesbimser sglr tou fror ginve myexs\n",
      "Tonmr is mese nid fel tefoelnl, ta,n alr lisen moorulm the menm yhhigs, sto'e an silngec mhit:;\n",
      "S;\n",
      "ECI ULWINSRS CALNGBPYCEH:\n",
      "A\n",
      "Thhy broem ceotue;\n",
      "Thed toe, meve itro ture a yp tet el ivero ie thet nopeec, whast sy, tee moe aosf caa wef okannrec, ny he fo fihbasde lad bikrvha. Npleud;\n",
      "Be sugi cora shi gtoo:\n",
      "Thee seho rhos ge, seuy ho,e hond tit prhe chw af, annte toum lilnnk cisiy tou tis mng ti farilliuo t\n"
     ]
    }
   ],
   "source": [
    "m = BiGramWithMultiHeadSelfAttentionSingleBlock()\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(5000):  # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "print(\n",
    "    decode(\n",
    "        m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b426c-a701-4ab7-a92e-961fa445b1fb",
   "metadata": {},
   "source": [
    "```txt\n",
    "problems with this\n",
    "this NN got bigger, has problems with backpro\n",
    "two ideas which help a lot with this big networks \n",
    "1. Residual connections\n",
    "2. layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7956c4-fa47-4d16-ae79-841c0cb6e258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5]) Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n",
      "torch.Size([5]) Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# layernorm ELI5\n",
    "\n",
    "a = torch.randn(10, 5)\n",
    "ln = nn.LayerNorm(5)\n",
    "a_normed = ln(a)\n",
    "a_normed.mean(1), a_normed.std(1)\n",
    "\n",
    "for p in ln.parameters():\n",
    "    print(p.shape, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c966ac32",
   "metadata": {},
   "source": [
    "\n",
    "## <a id=\"bigram-modified-multi-block\"></a> Multi Self Attention Block with layernorm and skip connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63565802-e7a8-4051-ac49-68b3df6dfe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BlockModified = Block + layernorm + skip connection(residual connection)\n",
    "\n",
    "\n",
    "class BlockModified(nn.Module):\n",
    "    def __init__(self, input_size, head_size, num_heads):\n",
    "        super(BlockModified, self).__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Head(input_size, head_size // num_heads) for i in range(num_heads)]\n",
    "        )\n",
    "        self.ffn = nn.Linear(head_size, head_size)\n",
    "        self.ln1 = nn.LayerNorm(head_size)\n",
    "        self.ln2 = nn.LayerNorm(head_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        outputs = [head(x) for head in self.heads]\n",
    "        outputs = torch.concat(outputs, dim=-1) + x\n",
    "        outputs = self.ln1(outputs)  # layernorm-1\n",
    "        outputs = self.ffn(outputs) + x  # skip connection\n",
    "        outputs = self.ln2(outputs)  # layernorm-2\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0fef75-8e47-42c8-8ba8-cab03b90358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGramWithModifiedBlock(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BiGramWithModifiedBlock, self).__init__()\n",
    "        emb_size = 16\n",
    "        self.token_embed_table = nn.Embedding(vocab_size, emb_size)\n",
    "        self.position_embed_table = nn.Embedding(block_size, emb_size)\n",
    "        self.blocks = nn.Sequential(\n",
    "            BlockModified(emb_size, emb_size, num_heads=4),\n",
    "            BlockModified(emb_size, emb_size, num_heads=4),\n",
    "            BlockModified(emb_size, emb_size, num_heads=4),\n",
    "            BlockModified(emb_size, emb_size, num_heads=4),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(emb_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # x - > (B,T)\n",
    "        B, T = x.shape\n",
    "        token_embeddings = self.token_embed_table(x)  # B, T, emb_size\n",
    "        positional_emebddings = self.position_embed_table(\n",
    "            torch.arange(T)\n",
    "        )  # B,T, emb_size\n",
    "        logits = (\n",
    "            token_embeddings + positional_emebddings\n",
    "        )  # B,T, emb_size # no change in dimensions\n",
    "        logits = self.blocks(logits)  # B, T, emb_size\n",
    "        # thing to observer - size is same after applying multiheadattention blocks\n",
    "        # how to apply layer norm here? - better to apply internally?\n",
    "        logits = self.lm_head(logits)  # B, T, vocab_size\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            logits = logits.view(B * T, -1)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        B, T = idx.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            # take the last one\n",
    "            logits = logits[:, -1, :]  # B,C\n",
    "            probs = F.softmax(logits, -1)\n",
    "            new_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, new_idx], axis=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36a16d0-17db-4ee8-8007-008cdf14bf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.361734628677368\n",
      "\n",
      "\n",
      "Su I't pint That barver, thed thep, gody! omly mat o poul so mpy thir wam they:\n",
      "And that end.\n",
      "\n",
      "\n",
      "K':\n",
      "Fallrcewe for ther thamude: man.\n",
      "\n",
      "I ing\n",
      "Thas thag.\n",
      "\n",
      "Who gote fonase meme,':\n",
      "I I shel thcae bon!\n",
      "\n",
      "RLOLUCR:\n",
      "Thid rathertuefrt koy, theay ust'd and thad.\n",
      "\n",
      "BBEFINDIMGOHEOHENRS:\n",
      "Ighad beved ent not ant;\n",
      "Ina monoust e nof twond gor woa nome hin ind hape then to nave coir ist cart so reave\n",
      "And has'm urme o,\n",
      "An tha thou' so suf,\n",
      "Ere thin\n",
      "I ing, Vocy my ance tint atth! tis's; ands hing dep: ast tUExs bre \n"
     ]
    }
   ],
   "source": [
    "m = BiGramWithModifiedBlock()\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(5000):  # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "print(\n",
    "    decode(\n",
    "        m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[\n",
    "            0\n",
    "        ].tolist()\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
