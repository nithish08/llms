# llms
Document understading of LLMs


- [ ] implement the Tranformers + Multi head tranformers mentioned here - https://www.youtube.com/watch?v=kCc8FmEb1nY&t=2215s

    Understanding the transformers

    - [x] Implement Bigram Model
    - [x] Implement self attention logic in adhoc fashion
    - [x] Implement self attentio  adhoc logic + train this self attention block to the bigram model
    - [ ] Implement the multi head self attention and train the network

- [ ] Read through the coursera genai course finsihed materials.

- [ ] Get an overview for the curretn status



